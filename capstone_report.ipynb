{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project : Predicting US High School Graduation Success\n",
    ">Author: jtmoogle @github.com  Email: jtmoogle@gmail.com  \n",
    "Date: May 11, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to predict the success if US High School Graduates can meet 90% graduation rate in 2020[1].  In this proposal, we leveraged the publicly available [data sources for AT&T 2015 'Data for Diplomas' Hakathon](https://datafordiplomas.devpost.com/details/resources)[2], and used machine learning/ML techniques listed in Table 1.\n",
    "\n",
    "*Table 1. Classification and Regression algorithms to evaluate problems *\n",
    "\n",
    "| Problem Statement | Pattern ML | Predictive Model |  \n",
    "| -------------------- | --------------- | \n",
    "| 1. **Prediction if achieving 90% graduation goal** | Supervised Learning | Classification model | \n",
    "| 2. **Prediction of graduation rate** | Supervised Learning | Regression model | \n",
    "\n",
    "*Tasks* were performed below\n",
    "- Modeled and predicted individual performance of school district\n",
    "- Compared predictive accuracy of different algorithms\n",
    "- (Stretch target) If the feature is suitable for classification, a further comparative analysis was done that would be reusable/useful for regression.\n",
    "\n",
    "*Goal* was to explore insights, and understand the factors related to success and failure, so we could propose actionable plans to increase the U.S. high school graduation rate reaching 90% by 2020[1]\n",
    "\n",
    "\n",
    "Because we had very large number of features, we preprocessed and performed feature selections to extract those features that were really important rather than fitting a complete feature set of predictive models.\n",
    "\n",
    "The inputs were chosen by performing various feature selection algorithms for classification and regression,  respectively.  As a result of feature selections, we obtained less features which were very important for fitting predictive models.\n",
    "\n",
    "Through the feature selection, we allowed the ML algorithms to train faster, reduce training time and evaluation time, reduce the complexity of predictive models, improve the accuracy of models, and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "Children are our future, higher education bring children better fortune. Our kids could benefit from the goal of \"Increase high school graduation rate in 2020 to 90% with a \"better future.\"\n",
    "\n",
    "For decades, the high school graduation rate increased from 71.7% in 2001 to 81.4% in 2013 nationally [2].  People used a variety of machine learning techniques and predictive analytics, so they gain actionable insights and improve educational opportunities early.  An example from Tacoma public schools in Washington State, which uses the ML models to predict the risk of dropping out from schools, so teachers could intervene early to help students at risks.  In doing so, graduation rates increased from 55% to 78% by 2014[4].\n",
    "\n",
    "By 2020, we need more 9% (about 310,000 more) graduates to meet our goal of 90% on-time graduation rate by 2020.  Students in low-income, minority, and special education students, big cities/big districts, and big states seems struggle and challenge to reach graduation rate 90% on-time [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "This proposal used the supervised ML techniques of classification and regression models to evaluate problems below:\n",
    "\n",
    "1. Prediction if high schools have a success or failure of 90% or more graduates on-time\n",
    "    This was in supervised *classification* process.  \n",
    "2. Prediction of high school graduation rate \n",
    "    This was in supervised *regression* process.\n",
    "Goals\n",
    "- Explore potential factors or insights: significant impact of features on the graduation rates, and seek potential linkages\n",
    "- Understand factors related to success and failure: evaluate the performance and predictive power of fitting models in the school district level \n",
    "\n",
    "Experimental Tasks\n",
    "- Feature selection: identify which predictor (feature/variables) really matter to establish a model that accurately predicts at or exceed 90% graduation rate  \n",
    "- Model and predict individual performance of school district \n",
    "- Compare prediction accuracy of different algorithms\n",
    "- (Stretch target) conduct a further comparative analysis if the feature is suitable for classification, that was reusable/useful for regression.\n",
    "\n",
    "Analyzing the results may lead to the ideas of increasing graduation rates.  In doing so, we can act on before problem becomes serious, and can propose workable solutions feasible to bring our goal closer to 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "#### Evaluation Metrics\n",
    "For **Classification metrics**, we calculated average/mean, standard deviation of F-score (Precision, Recall) used for finding the correctness and accuracy of the model.  The best value at 1 and worse score at 0\n",
    "\n",
    "*Table 2. Possible results of prediction *\n",
    "\n",
    "| Confusion Matrix | Predicted as True | Predicted As False |\n",
    "| :---: | :----: | :----: |\n",
    "|Actually True |  True Positive (TP) | False Negative (TN) |\n",
    "|Actually False| False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "*Equations*\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy =  \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Precision =  \\frac{TP}{TP+FP}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Recall =  \\frac{TP}{TP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "F = 2 \\cdot \\frac{1}{\\tfrac{1}{\\mathrm{recall}} + \\tfrac{1}{\\mathrm{precision}}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "For **Regression metrics**, we calculated Root Mean Squared Error (RMSE) used measure the difference between the predicted by a model and actual value observed, how accurately the model predicts the response.  \n",
    "\n",
    "Square root of the average of squared errors\n",
    "\\begin{equation}\n",
    "{RMSE}=\\sqrt\\frac{1}{n}{\\sum_{i=1}^n (p_i - p)^2}  \\:\\:\\:\\:\\: where\\: p_i\\: is\\: predicted\\: value, p\\: is\\: modeled\\: value\n",
    "\\end{equation}\n",
    "\n",
    "Lower values of RMSE indicate better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.Analysis\n",
    "### Data Exploration\n",
    "We used the 2011-2012 high school graduates and 2010 census information per school district, state, and county level for our experiment.  \n",
    "\n",
    "1. Data Collection: the *GRADUATION_WITH_CENSUS.csv* was manually pre-downloaded from [Data for Diplomas_Merged Data.zip](https://challenges.s3.amazonaws.com/data_for_diplomas/Data for Diplomas_Merged Data.zip) located at https://challenges.s3.amazonaws.com/data_for_diplomas/Data for Diplomas_Merged Data.zip, and extracted CSV file and its  definition ALL_DATA_SCHEMA_M.pdf\n",
    "    - The dataset  contain 2012 Four-year regulatory Adjusted Cohort Graduation Rates (ACGR) joined with the maximum overlapping [2010 Census data](https://challenges.s3.amazonaws.com/data_for_diplomas/Data%20for%20Diplomas_Unmerged%20Data.zip)\n",
    "    - The analysis was focus on the year of 2011/2012\n",
    "2. Exploratory Analysis: our first data exploration found the money fields such as \"Med_HHD_Inc_ACS_08_12\", \"Med_HHD_Inc_ACSMOE_08_12\", \"Aggregate_HH_INC_ACS_08_12\",  \"Aggregate_HH_INC_ACSMOE_08_12\", \"Med_House_value_ACS_08_12\",  \"Med_House_value_ACSMOE_08_12\", \"Aggr_House_Value_ACS_08_12\", \"Aggr_House_Value_ACSMOE_08_12\" prefixed with a dollar sign ($) and embedded with commas (,).  Cleaning step included removing the dollar sign and commas, and store the tidy dataset to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "The GRADUATION_WITH_CENSUS data contained various counts and percentages for each school district's population, ethnicity, gender, age, geographic location, and family educational background, and household etc.   THe referenced fields were defined in [ALL_DATA_SCHEMA_M.pdf](https://github.com/jtmoogle/capstone-proposal/blob/master/proposal-data-analysis.pdf) \n",
    "\n",
    "Because we had 576, very large number of features , we pre-processed and performed feature selections to extract those features that were really important rather than fitting a complete feature set of predictive models.  \n",
    "\n",
    "Actual inputs were the result of feature selections after performing various feature selection algorithms for classification and regression, and obtained less features that were very important for fitting predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Variable\n",
    "The output variable represent the high school district performance on graduation\n",
    "1. **Classification** target variable is \"Success_Pass_90\" indicator derived from \"ALL_RATE_1112\"\n",
    "    - integer, value 1, PASSED successful if ALL_RATE_1112 is equal to or greater than 90.0\n",
    "    - integer, value 0, NOT PASSED successful if ALL_RATE 1112 is less than 90.0\n",
    "2. **Regression** target variable is \"ALL_RATE_1112'\n",
    "    - float number, in range of 0 to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Statistics \n",
    "We had 9,907 observations, and 576 features each observation in 2011-2012 graduation cohort.  The input data contained the count and percentage for population, ethnicity, gender, age, geography, and family educational background, household by each school district.  \n",
    "\n",
    "- By Race total students in the 2011-2012 graduation cohort  \n",
    "        ALL_COHORT_1112             3,307,623   Total number of students in the 2011-2012 graduation cohort \n",
    "        MAM_COHORT_1112                30,965   Native Americans students\n",
    "        MAS_COHORT_1112               168,589   Asian/Pacific Islander students\n",
    "        MBL_COHORT_1112               553,003   Black/Africa American students\n",
    "        MHI_COHORT_1112               658,507   Hispanic students\n",
    "        MTR_COHORT_1112                46,676   Two/Three races students\n",
    "        MWH_COHORT_1112             1,835,800   White students\n",
    "        CWD_COHORT_1112               393,902   Children with disabilities\n",
    "        ECD_COHORT_1112             1,431,050   Economically disadvantaged\n",
    "- By Geography & Population total count\n",
    "        Tot_Population_CEN_2010     41,000,304  U.S. resident population in 2010 Census\n",
    "        RURAL_POP_CEN_2010          19,256,868  Population living outside of an Urbanized Area or Urban Cluster \n",
    "        URBANIZED_AREA_POP_CEN_2010 13,311,089  Population living in a densely settled area containing 50,000 or more people\n",
    "        URBAN_CLUSTER_POP_CEN_2010   8,432,347  Population living in a densely settled area containing 2,500 to 49,999 people\n",
    "- By Gender total count \n",
    "        Males_CEN_2010              20,353,022  Males\n",
    "        Females_CEN_2010            20,647,282  Females\n",
    "- By Age total count \n",
    "        Pop_under_5_CEN_2010         2,571,831  Persons less than age 5 in 2010 census\n",
    "        Pop_5_17_CEN_2010            7,327,965  Persons aged 5 to 17 in 2010 census\n",
    "        Pop_18_24_CEN_2010           3,608,431  Persons aged 18 to 24 in 2010 census\n",
    "        Pop_25_44_CEN_2010           9,898,820  Persons aged 25 to 44 in 2010 census\n",
    "        Pop_45_64_CEN_2010          11,477,346  Persons aged 45 to 64 in 2010 census\n",
    "        Pop_65plus_CEN_2010          6,115,911  Persons aged 65 and over in 2010 census\n",
    "- By Family Background  total count \n",
    "        Pov_Univ_ACS_08_12          39,675,431  People under 15 years old\n",
    "        Prs_Blw_Pov_Lev_ACS_08_12    5,457,898  People classified as below the poverty level\n",
    "        Civ_labor_16plus_ACS_08_12  20,216,951  Civilians aged 16 years and over and in the labor force\n",
    "        Civ_emp_16plus_ACS_08_12    18,543,130  Civilians aged 16 years and over and employed\n",
    "        Civ_unemp_16plus_ACS_08_12   1,673,821  Civilians aged 16 years and over and unemployed\n",
    "        Civ_labor_16_24_ACS_08_12    2,833,010  Civilians aged 16 to 24 in the labor force in\n",
    "- By Family Education total \n",
    "        Not_HS_Grad_ACS_08_12        3,730,293  People 25 years old and over who are not high school graduates\n",
    "        College_ACS_08_12            6,266,894  People 25 years old and over with college degree or higher\n",
    "- By English Language Speaks (Households where NO One age 14 & over speaks English only/very well) Total for\n",
    "        ENG_VW_ACS_08_12               337,275  Households where NO One age 14 & over speaks English only/very well\n",
    "        ENG_VW_SPAN_ACS_08_12          225,915  Spanish or Spanish Creole language\n",
    "        ENG_VW_INDO_EURO_ACS_08_12      55,547  Indo-European language \n",
    "        ENG_VW_API_ACS_08_12            43,778  Asian and Pacific Island language\n",
    "        ENG_VW_OTHER_ACS_08_12          12,035  Language other than English, Spanish, Indo-Euro, or API\n",
    "- By Income Total \n",
    "        PUB_ASST_INC_ACS_08_12                380,658  Households that receive public assistance income\n",
    "        Med_HHD_Inc_ACS_08_12             515,609,190  Median ACS household income\n",
    "        Aggregate_HH_INC_ACS_08_12  1,035,306,469,500  Sum of all incomes in the household\n",
    "- Other Total \n",
    "        Born_US_ACS_08_12           38,404,354  People were citizen of US at birth\n",
    "        Born_foreign_ACS_08_12       2,492,610  People were not citizen of US at birth\n",
    "        US_Cit_Nat_ACS_08_12         1,062,900  People were citizen of US through naturalization\n",
    "        NON_US_Cit_ACS_08_12         1,429,710  People were not citizen of US  \n",
    "        MrdCple_Fmly_HHD_CEN_2010    8,326,407  Married-couple family households\n",
    "        Not_MrdCple_HHD_CEN_2010     7,361,715  Households with no Married Couple presented\n",
    "        Female_No_HB_CEN_2010        1,728,876  Households with a female householder, no husband  \n",
    "        NonFamily_HHD_CEN_2010       4,890,585  Households lives alone or with nonrelatives only\n",
    "\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.load_gradcensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**\n",
    "![Figure 1: Graduation Census by goal](saved/plot_cls_gradcensus.1.png) Figure 1. Graduation Census by Goal.  \n",
    "- 3,857 (39%) school districts have (1=PASSED) met the goal of 90% graduation rate\n",
    "- 6,050 (61%) school districts has NOT met (0=NOT PASSED) the goal yet.\n",
    "\n",
    "![Figure 2: Graduation Census by State](saved/plot_cls_gradcensus.2.png) Figure 2. Graduation Census by State.  \n",
    "- 48 states, 320 counties, 3,307,632 high school students.  (1=PASSED) The school districts have met the goal of 90% graduation rate; (0=NOT PASSED) school districts has NOT met yet. \n",
    ">Reference: API implementation: jtmoogle.hsgraduation.plot_cls_gradcensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**\n",
    "![Figure 3: Regression Distribution of ALL_RATE_1112](saved/plot_rgs_gradcensus.png) Figure 3. Visualize Regression distribution of ALL_RATE_1112.  \n",
    "- The Kernel Density Estimate/KDE Density plot showed all school districts on top of the corresponding histogram\n",
    "- The rug on the x-axis showed actual data points.  Observed overcrowded around 50%, 83-85% graduation rates\n",
    "- y-axis represented the probability density per unit on the x-axis.\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.plot_rgs_gradcensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "The two processes trained a set of predictive models respectively for classification and regression. The best candidate selection of supervised learning algorithms experimented.  These models were evaluated using different performance metrics for classification and regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Table 3 Listed predictive models, feature selection methods, training and test sample counts by target variable*\n",
    "\n",
    "| Predictive Model <br> Target Variable | Feature Selection <br>Reduced to less variables | Samples| Evaluated Models | Metric Result Comparison | \n",
    "| --------------- | --------------- | --------------- | :----: | :----: | --------------- | ---- |\n",
    "| Classification model <br> Target:Success_Pass_90 | **RFE model with Logistic Regression** <br> Reduce to **20** |Training **6,920 / 70%** <br>Testing  **2,966 / 30%** | **Gaussian Naive Bayes (NB)<br>Logistic Regression<br>Random Forest(RF)<br>K-Nearest Neighbors (KNN)<br>Decision Tree(DT)<br>Ridge Classifier(Ridge)<br>Perceptron<br>Gradient Boosting Classifier(GB)** | **F score** | \n",
    "| Regression model<br> Target:ALL_RATE_1112 | **Stepwise Feature Selection** <br>Reduce to **30** | Training **6,828 / 70** <br> Testing **2,927 / 30%** | **Linear Regression(Linear)<br>Random Forest(RF)<br>k-Nearest Neighbors(KNN)<br>Decision Tree(DT)<br>Ridge<br>Support Vector Regression(SVR)** | **Root Mean Square Error(RMSE)** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification** \n",
    "1. Feature Selection: selected good features through the feature selection, we used various ML algorithms to rank the feature importance, scaled the result in range of zero and one, and then calculated the mean/average of results.  \n",
    "    - Firstly used the Recursive Feature Elimination/RFE with the estimator of Random Forest Classifier to fit the regression models.  This process iterated reserving the best performance features and removing the worse features until all features in database were exhausted.  \n",
    "        - Default to selected the 20th best ranked as input features for classification model\n",
    "        - Used Random Forest Classifier to get the best feature importance and the best accuracy score for classification features\n",
    "- In addition, compared the result like coef or feature importance from the algorithms of Logistic Regression/Logit, Ridge Classifier, Decision Tree/DT, Random Forest Classifier, and average of mean of all ranking scores.  We re-selected features if the mean of result was greater than 0.2 in range of 0 and 1. \n",
    "- Create Samples: the dataset was split into 70% training, 30% testing using the target variable.  We used the training dataset to learn and evaluate predictive models, and used the testing dataset to measure the performance metric.\n",
    "    - Training set was 6920 (70.00%)  Testing 2966  (30.00%)  in total 9886.\n",
    "- Predictive Models: used the following models to train/learn data, reduced training and evaluation time, reduced the complexity of predictive models, improve the accuracy of models and reduce overfitting.\n",
    "    - Gaussian Naive Bayes (NB)\n",
    "    - Logistic Regression\n",
    "    - Random Forest (RF)\n",
    "    - K-Nearest Neighbors (KNN)\n",
    "    - Decision Tree (DT)\n",
    "    - Ridge Classifier (Ridge)\n",
    "    - Perceptron\n",
    "    - Gradient Boosting Classifier (GB)\n",
    "- F score was used for metric result comparison\n",
    "\n",
    "\n",
    "**Regression**\n",
    "1. Feature Selection selected good features performed a stepwise feature selection based on p-value from statsmodels.api.OLS [9].  Iterate calculating, evaluating\n",
    "    - if p-value was greater than threshold out (0.05), the feature which reduced the performance the least\n",
    "    - if p-value was less than threshold in (0.01),the feature which improved performance the most\n",
    "    - Forward selected the best single feature, which improved performance the most.  \n",
    "    - Backward elimination, a set of remaining features, repeatedly delete the feature that reduces performance the least\n",
    "    - Used Random Forest Regressor to get the best feature importance and the best R2 score for Regression  features\n",
    "- In addition, compared the result like coef or feature importance from the algorithms of Linear Regression, Ridge Regressor, Decision Tree/DT, Random Forest/RF Regressor, and average of mean of all ranking scores.  We re-selected features if the mean of result was greater than 0.2 in range of 0 and 1. \n",
    "- Create Samples: the dataset was split into 70% training, 30% testing using the target variable.  We used the training dataset to learn and evaluate predictive models, and used the testing dataset to measure the performance metric.\n",
    "    - Training set was 6,828 (69.99%) testing 2,927  (30.01%)  in total 9,755\n",
    "- Predictive Models: used the following models to train/learn data faster, reduced training and evaluation time, reduced the complexity of predictive models, improve the accuracy of models and reduce overfitting.\n",
    "    - Linear Regression (Linear)\n",
    "    - Random Forest (RF)\n",
    "    - k-Nearest Neighbors (KNN)\n",
    "    - Decision Tree (DT)\n",
    "    - Ridge\n",
    "    - Support Vector Regression (SVR)\n",
    "- Root Mean Square Error/RMSE was used for metric result comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "We used the naive technique to make a forecast and calculate the baseline performance.  After we explored more options for naive prediction during capstone report, we decided to adopt different option.  The model results were compared with the naive predictor benchmarks.  We selected the best performance and the less training time for the final model.\n",
    "1. Use Gaussian Naive Bayes (NB) for classification \n",
    "- Used Linear Regression for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "### Data Processing\n",
    "\n",
    "#### Pre-process Data \n",
    "We applied the following logics on the 576 features, and reduced to 152 features.  \n",
    "1. Removed non-relevant variables toward target variable.  Dropped the columns whose feature names contained the partial/full text of *MOE_, _FRMS_, Mail, Percentage, County, State, Tract, District, GIDTR, Tract, Flag, Response, Delete, Vacant, BILQ, Diff, Leave, Plumb.*\n",
    "2. Included columns whose feature names contained partial/full text of *Inc, INC, _COHORT_, pct_, avg_, _House_, _AREA_, ALL_, Success*\n",
    "3. Selected columns whose data types were float64 or int32/int64.  \n",
    "4. Dropped rows if column has NaN/nullable values\n",
    "5. Extracted target data based on the target column name\n",
    "6. Removed non-relevant variables such as the target column: (1) Success_Pass_90 for classification (2) ALL_RATE_1112 for regression\n",
    "7. Identified missing value and imputed NaN with zero by filling with zero value\n",
    "\n",
    "- We did not do further data transformation because AT&T group has consolidated and applied necessary transformation for Hackathon in advanced.\n",
    "\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.preprocdata, preproc_cls_data, preproc_rgs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Implementation of APIs\n",
    "We implemented various supervised learning algorithms respectively two process for classification and regression.  The following APIs invoked in this report were implemented in Python, and source files were on GitHub [jtmoogle](https://github.com/jtmoogle/capstone-report/jtmoogle) \n",
    "- helper.py contains classes as static methods\n",
    "    - MyLogger - logging to a file or console output for purpose of debug, info, error \n",
    "    - MyHelper - common functions to load dataset, print out statistic summary, save to result to a file\n",
    "- hsgraduation.py contains functions and methods \n",
    "    1. load_gradcensus - load jtmoogle/data/GRADUATION_WITH_CENSUS.csv\n",
    "    2. Classification  \n",
    "        (1) plot_cls_gradcensus - illustrate classification graduation census data  \n",
    "        (2) preproc_cls_data - prepare classification data cleaning   \n",
    "        (3) cls_feature_sel - classification feature selection  \n",
    "        (4) compare_cls_featranking - compared classification feature ranking\n",
    "        (5) cls_stats - classification statistic summary  \n",
    "        (6) cls_acc_featimportance - mean accuracy score and feature importance result for classification feature\n",
    "        (7) create_cls_sample - create sample for Training and Testing datasets  \n",
    "        (8) cls_acc_featimportance - calculated F score, accuracy, feature importance  \n",
    "        (9) cls_visual_benchmark - benchmark result and visualization  \n",
    "        (10) handy functions:  cls_pca - PCA result for classification   \n",
    "    3. Regression  \n",
    "        (1) plot_rgs_gradcensus - illustrate regression graduation census data  \n",
    "        (2) preproc_rgs_data - prepare Regression data cleaning  \n",
    "        (3) rgs_feature_sel - regression feature selection  \n",
    "        (4) compare_rgs_featranking - compared regression feature ranking\n",
    "        (5) rgs_stats - regression statistic summary  \n",
    "        (6) create_rgs_sample - create sample for Training and Testing datasets  \n",
    "        (7) rgs_r2_featimportanct - R2 score and feature importance for regression features  \n",
    "        (8) rgs_visual_benchmark - benchmark result and visualization  \n",
    "- runme.py which was capable to reproducible statistics summary results, and visualization seen in this report\n",
    "- data/GRADUATION_WITH_CENSUS.csv (raw data)\n",
    "    GRADUATION_WITH_CENSUS.csv.ds.csv (cleaner data, output data file via load_gradcensus method)\n",
    "    ALL_DATA_SCHEMA_M.pdf (field definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation for Classification \n",
    "#### Feature Selection: Select Good Features \n",
    "We implemented the wrapper method of *RFE/Recursive Feature Elimination using sklearn.feature_selection API.  The task was repeated as follows\n",
    "1. Constructed the Logistic Regression model estimator\n",
    "- Chose the best performance features \n",
    "- Removed the worse ones.  \n",
    "\n",
    "The process iterated until all features in dataset were exhausted.  This was eliminated from 152 to 20 features, and value of 1 was ranked the 20th best features which were selected as input variables for classification model.  \n",
    "\n",
    "In addition, we used various models to evaluate these 20 features, obtained coefficient or feature importance values per feature, calculated the mean/average of coefficient or feature importance, and then compared side by side.  At last, we selected features with better averages greater than 0.2.  This reduced to 8 features with better important correlation\n",
    "\n",
    "\n",
    "*Table 4 Compared result of various feature selection methods. Chose features with better averages greater than 0.2*\n",
    "\n",
    "|Features|DT|RF|Ridge|logit|Mean| Feature Category | Feature Description |\n",
    "| --- | --- | ---| ---| ---| ---|---| ---|\n",
    "|MWH_COHORT_1112|1|1|0|0.02|**0.5**|HS Graduation | Number of White students in the graduation cohort|\n",
    "|ECD_COHORT_1112|0.93|0.41|0.03|0.05|**0.36**|HS Graduation |Number of economically disadvantaged students in the graduation cohort|\n",
    "|Med_HHD_Inc_ACS_08_12|0.23|0.18|0|1|**0.35**|Household |Median ACS household income |\n",
    "|pct_Female_No_HB_CEN_2010|0|0.08|1|0|**0.27**|Household | Households with a female householder, no husband present |\n",
    "|pct_College_ACS_08_12|0.03|0.09|0.91|0|**0.26**|Population|Persons 25 years and over with college degree or higher |\n",
    "|pct_Not_MrdCple_HHD_CEN_2010|0.08|0.09|0.86|0.01|**0.26**| Household | Households with no Married Couple present|\n",
    "|pct_Tot_Occp_Units_ACS_08_12|0.05|0.16|0.83|0.01|**0.26**|Housing Unit| Total Occupied Housing Units which has regular occupants in ACS |\n",
    "|pct_Civ_emp_16p_ACS_08_12|0|0.07|0.75|0.01|**0.21**|Population|Civilians aged 16 years and over and employed |\n",
    "|pct_NH_White_alone_CEN_2010|0.02|0.03|0.69|0|0.18|Population|no Hispanic origin and their only race as \"White\" as Irish, German, Italian, Lebanese, Arab, Moroccan, or Caucasian|\n",
    "|pct_Prs_Blw_Pov_Lev_ACS_08_12|0.01|0.16|0.43|0|0.15|Population|Populateion classified as below the poverty level |\n",
    "|pct_Hispanic_CEN_2010|0|0|0.45|0|0.11|Population| Mexican, Puerto Rican, Cuban, or another Hispanic, Latino, or Spanish origin|\n",
    "|Aggregate_HH_INC_ACS_08_12|0|0.35|0|0|0.09|Household|Sum of all incomes in the Household|\n",
    "|CWD_COHORT_1112|0|0.23|0.06|0.01|0.08|HS Graduation|Number of children with disabilities in the graduation cohort|\n",
    "|MBL_COHORT_1112|0.06|0.19|0.02|0.02|0.07|HS Graduation|Number of Black students in the graduation cohort|\n",
    "|MHI_COHORT_1112|0.12|0.07|0.04|0.02|0.06|HS Graduation|Number of Hispanic students in the graduation cohort|\n",
    "|MAM_COHORT_1112|0.01|0.01|0.12|0|0.04|HS Graduation|Native American students in the graduation cohort|\n",
    "|LEP_COHORT_1112|0.01|0.09|0.01|0.01|0.03|HS Graduation|Number of limited English proficient students in the graduation cohort| \n",
    "|MAS_COHORT_1112|0.04|0.03|0.04|0|0.03|HS Graduation|Number of Asian/Pacific Islander students in the graduation cohort|\n",
    "|pct_Not_HS_Grad_ACS_08_12|0.03|0.03|0.02|0|0.02||People 25 years old and over who are not high school graduates|\n",
    "|Aggr_House_Value_ACS_08_12|0|0.02|0|0|0|Housing Unit|Aggregate housing unit value|\n",
    "\n",
    "\n",
    "![Figure 4: Classification Features and Mean Ranking Comparison Matrix](saved/cls_compare_ranking.png) Figure 4: Classification Features and Mean Ranking Comparison Matrix.    MWH_COHORT_1112 had the highest ranking mean value with 0.50; ECD_COHORT_1112 0.36; Med_HHD_Inc_ACS_08_12 0.35\n",
    "\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.cls_feature_sel, compare_cls_featranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ordinary Least Square/OLS Result for classification in table 5 listed overall model fit 9,894 observation.  Degree of freedom 9,874, and fitting model with 8 predictors\n",
    "\n",
    "- If P>|t| is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship, correlation between the dependent (target) variable and the independent (predictor) variables.\n",
    "- R-squared 0.467 overall measure of the strength of association in the dependent (target) variable 'Success_Pass_90' can be explianed by the independent variables (20 predictors)\n",
    "\n",
    "\n",
    "*Table 5. Classification: Summary of OLS Result for the dependent variable at the top (Success_Pass_90) with the 8 selected features as predictor variables below in Parameter Estimates.*\n",
    "\n",
    "                                 OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:        Success_Pass_90   R-squared:                       0.463\n",
    "    Model:                            OLS   Adj. R-squared:                  0.463\n",
    "    Method:                 Least Squares   F-statistic:                     1065.\n",
    "    Date:                Sat, 12 May 2018   Prob (F-statistic):               0.00\n",
    "    Time:                        21:15:56   Log-Likelihood:                -6291.3\n",
    "    No. Observations:                9886   AIC:                         1.260e+04\n",
    "    Df Residuals:                    9878   BIC:                         1.266e+04\n",
    "    Df Model:                           8                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ================================================================================================\n",
    "                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------------------------\n",
    "    MWH_COHORT_1112              -3.173e-05   1.56e-05     -2.029      0.043   -6.24e-05   -1.07e-06\n",
    "    ECD_COHORT_1112              -5.833e-05   8.34e-06     -6.991      0.000   -7.47e-05    -4.2e-05\n",
    "    Med_HHD_Inc_ACS_08_12         1.685e-06   3.99e-07      4.227      0.000    9.03e-07    2.47e-06\n",
    "    pct_Female_No_HB_CEN_2010       -0.0035      0.001     -3.053      0.002      -0.006      -0.001\n",
    "    pct_Not_MrdCple_HHD_CEN_2010    -0.0046      0.001     -7.673      0.000      -0.006      -0.003\n",
    "    pct_College_ACS_08_12            0.0043      0.001      7.979      0.000       0.003       0.005\n",
    "    pct_Tot_Occp_Units_ACS_08_12     0.0058      0.000     14.098      0.000       0.005       0.007\n",
    "    pct_Civ_emp_16p_ACS_08_12       -0.0002      0.000     -0.426      0.670      -0.001       0.001\n",
    "    ==============================================================================\n",
    "    Omnibus:                      276.301   Durbin-Watson:                   1.696\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1093.084\n",
    "    Skew:                           0.426   Prob(JB):                    4.36e-238\n",
    "\n",
    "When the following increase, Success_Pass_90 would (list first 3)\n",
    "- pct_Tot_Occp_Units_ACS_08_12:  increase 0.0058 \n",
    "- pct_College_ACS_08_12:   increase 0.0043\n",
    "- Med_HHD_Inc_ACS_08_12:   increase 1.685e-06\n",
    "\n",
    "When the following increase, Success_Pass_90 would (list first 3)\n",
    "- pct_Not_MrdCple_HHD_CEN_2010:  decrease 0.0046 \n",
    "- pct_Female_No_HB_CEN_2011:   decrease 0.0035\n",
    "- pct_Civ_emp_16p_ACS_08_12:   decrease 0.0002\n",
    "      \n",
    ">Reference: API implementation: jtmoogle.hsgraduation.cls_stats, cls_acc_featimportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Samples\n",
    "We used the training dataset to learn and evaluate predictive models.  The sklearn.model_selection.train_test_split API was used the target variable *Success_Pass_90* to split the dataset into 70% /6,920 observation for training, 30% / 2,966 observation for testing.  \n",
    ">Reference: API implementation: jtmoogle.hsgraduation.create_cls_sample\n",
    "\n",
    "#### Detect Outliers\n",
    "Using John Tukey's Method [35] to identfy outliers data points by calculating as 1.5 times the interquartile range (IQR). A data point with a feature that was beyond an outlier step outside of the IQR for that feature is considered abnormal.   For each feature, steps were \n",
    "1. To find the data points with extreme high or low values\n",
    "2. Calculated Q1 (25th percentile of the data) for the given feature\n",
    "3. Calculated Q3 (75th percentile of the data) for the given feature\n",
    "4. Computed Q3 - Q1 for IQR \n",
    "5. Used the IQR to calculate an outlier step (1.5 times the IQR).  Anything outside this range is an outlier\n",
    "\n",
    "\\begin{equation}\n",
    "[ Q_1 - k (Q_3 - Q_1 ) , Q_3 + k (Q_3 - Q_1 ) ] \\:\\:\\:\\:\\: where\\: k\\: is\\: 1.5\\: indicating\\: an\\: 'outlier'\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "All duplicated outlier data point would be removed from the sample, and stored data as 'good_data'\n",
    "\n",
    "#### Feature Scaling\n",
    "The natural logarithm was applied to scale the good_data, and we used heatmap to get further insight exhibited the feature's Correlation Matrix in Figure 5.  \n",
    "\n",
    "![Figure 5: Visualize comparing Classification data and log_data in Correlation Matrix](saved/cls_acc_featimportance.scaling.png) Figure 5. Visualized comparing Classification data and log_data in Correlation Matrix.   \n",
    "\n",
    "*Table 6 Feature Scaling: Log Transform change if any value above 0.5 *\n",
    "\n",
    "|Features in correlation above 0.5 | Value Change | Correlation Change|\n",
    "| --- | ---- | ----- |\n",
    "| ECD_COHORT_1112 & MWH_COHORT_1112 | 0.5 -> 0.7 | Stronger correlation |\n",
    "| pct_Not_MrdCple_HHD_CEN_2010 & pct_Female_No_HB_CEN_2010  | 0.6 -> 0.6 | no change | \n",
    "| pct_College_ACS_08_12 & Med_HHD_Inc_ACS_08_12 | 0.7 -> 0.7 | no change | \n",
    "| pct_Female_No_HB_CEN_2010 & ECD_COHORT_1112 | 0.2 -> 0.5 | Stronger |\n",
    "\n",
    "\n",
    "#### Fit Predictive Model for Supervised Learning Algorithms\n",
    "We used the training dataset to learn and evaluate predictive models.  The process trained a set of predictive models respectively, and experimented the sklearn.ensemble, sklearn.neighbors and sklearn.linear_model APIs as follows: \n",
    "- Gaussian Naive Bayes (NB) used default value\n",
    "- Logistic Regression (Logit) used default value\n",
    "- Random Forest Classifier (RF) used 5 for max_depth, 10 for n_estimators\n",
    "- K-Nearest Neighbors Classifier (KNN) used 3 for n_neighbors\n",
    "- Decision Tree Classifier (DT) use 5 for max_depth\n",
    "- Ridge Classifier (Ridge) used le-2 for tol, lsgr for solver\n",
    "- Perceptron used 50 for n_iter, 0.1 for alpha, none for penalty\n",
    "- Gradient Boosting Classifier(GB) used 100 for n_estimators, 4 for max_leaf_nodes, none for max_depth, 2 for random_state, 5 for min_samples_split\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.cls_acc_featimportance\n",
    "\n",
    "### Refinement\n",
    "The Gaussian Naive Bayes (NB) was selected as Naive Prediction.  We compared the results of performance metrics with the Naive Prediction.  Time taken for training and testing were captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation  for  Regression \n",
    "#### Feature Selection: Select Good Features \n",
    "We performed the *Stepwise feature selection based on p-value from statsmodels.api.OLS [9]* for regression model.  The task was iterated calculating and evaluating the following logics.  \n",
    "1. Repeated if p-value was greater than threshold-out value (default to 0.05), the feature had reduced performance to the least\n",
    "- Repeated if p-value was less than threshold-in value (default to 0.01),the feature had improved performance the most\n",
    "- The forward path selected the best single feature which improved performance the most.  \n",
    "- The backward path eliminated a set of remaining features, repeatedly deleted the features that reduced performance the least\n",
    "\n",
    "As result of the process completion, features were reduced from 152 to 30 features which were selected as input variables for regression model.   \n",
    "\n",
    "Furthermore, we used various models to evaluate these 30 features, obtained coefficient or feature importance values per feature, calculated the mean/average of coefficient or feature importance, and then compared side by side. At last, we selected features with better averages greater than 0.2. This reduced to 13 features with better important correlation.\n",
    "\n",
    "*Table 7 Compared result of various feature selection methods. Selected features with better averages greater than 0.2*|\n",
    "\n",
    "|Feature|Corr|DT|Linear|RF|Ridge|Mean|Feature Category | Feature Description |\n",
    "|---|---|---|---|---|---|---|--|---|\n",
    "|pct_Tot_Occp_Units_CEN_2010|0.64|1|0.07|1|0.08|**0.56**|Housing Unit| Total Occupied Housing Units which has regular occupants in census |\n",
    "|pct_NH_NHOPI_alone_CEN_2010|0|0.12|1|0.09|1|**0.44**|Population| Non-Hispanic Native Hawaiian and Other Pacific Islander only |\n",
    "|Aggregate_HH_INC_ACS_08_12|1|0.54|0|0.44|0|**0.4**|Household| Sum of all incomes in the household |\n",
    "|pct_NH_AIAN_alone_ACS_08_12|0.46|0.55|0.19|0.46|0.18|**0.37**|Population| Non-Hispanic American Indian and Alaska Native only |\n",
    "|MBL_COHORT_1112|0.13|0.68|0|0.63|0|**0.29**|HS district | Number of Black students in the graduation cohort |\n",
    "|pct_Pop_Under_5_CEN_2010|0.08|0.09|0.61|0.14|0.51|**0.29**|Population | Number of persons under age 5 |\n",
    "|pct_Prs_Blw_Pov_Lev_ACS_08_12|0.69|0.32|0.04|0.32|0.04|**0.28**|Population | Number of people classified as below the poverty level |\n",
    "|pct_Age5p_WGerman_ACS_08_12|0.01|0|0.66|0|0.64|**0.26**|Population| People ages 5+ years  who speak English less than \"very well\" and speak *another* West Germanic language at home |\n",
    "|pct_PUB_ASST_INC_ACS_08_12|0.32|0.25|0.2|0.25|0.2|**0.24**|Household |\thouseholds that receive public assistance income |\n",
    "|Med_House_value_ACS_08_12|0.55|0.28|0|0.29|0|**0.22**| Housing Unit | Median of house value|\n",
    "|pct_Age5p_German_ACS_08_12|0.01|0.05|0.48|0.06|0.48|**0.22**|Population | Persons 5+ years who speak English less than \"very well\" and speak German at home |\n",
    "|pct_Females_CEN_2010|0.05|0.28|0.28|0.26|0.21|**0.22**|Population|Number of Females in total population|\n",
    "|pct_NH_Blk_alone_CEN_2010|0.37|0.22|0.13|0.24|0.11|**0.21**|Population| Non-Hispanic Black/African American only |\n",
    "|pct_Civ_emp_16p_ACS_08_12|0.1|0.33|0.05|0.36|0.06|0.18| | |\n",
    "|pct_Mobile_Homes_ACS_08_12|0.34|0.23|0.03|0.23|0.03|0.17|\n",
    "|pct_Pop_45_64_CEN_2010|0|0.27|0.17|0.2|0.18|0.16|\n",
    "|pct_Female_No_HB_ACS_08_12|0.19|0.21|0.08|0.22|0.06|0.15|\n",
    "|pct_HHD_PPL_Und_18_CEN_2010|0.06|0.16|0.17|0.16|0.12|0.13|\n",
    "|pct_NO_PH_SRVC_ACS_08_12|0.13|0.15|0.11|0.17|0.1|0.13|\n",
    "|pct_Pop_25_44_ACS_08_12|0.04|0.24|0.1|0.2|0.09|0.13|\n",
    "|pct_Rel_Under_6_CEN_2010|0.07|0.19|0.13|0.14|0.11|0.13|\n",
    "|pct_ENG_VW_INDOEURO_ACS_08_12|0|0.09|0.23|0.06|0.24|0.12|\n",
    "|pct_Hispanic_CEN_2010|0.03|0.21|0.05|0.25|0.04|0.12|\n",
    "|pct_NonFamily_HHD_CEN_2010|0.11|0.11|0.13|0.1|0.08|0.11|\n",
    "|pct_Census_UAA_CEN_2010|0|0.21|0.04|0.2|0.04|0.1|\n",
    "|pct_MLT_U10p_ACS_08_12|0.03|0.17|0.05|0.19|0.05|0.1|\n",
    "|pct_RURAL_POP_CEN_2010|0.38|0|0.02|0.1|0.02|0.1|\n",
    "|pct_Rel_Family_HHDS_CEN_2010|0.11|0.13|0.08|0.11|0.02|0.09|\n",
    "|URBANIZED_AREA_POP_CEN_2010|0.27|0.03|0|0.04|0|0.07|\n",
    "|pct_Inst_GQ_CEN_2010|0|0.06|0.09|0.1|0.07|0.06|\n",
    "\n",
    "![Figure 6: Regression Features and Mean Ranking Comparison Matrix](saved/rgs_compare_ranking.png) Figure 6: Regression Features and Mean Ranking Comparison Matrix.    pct_Tot_Occp_Units_CEN_2010 had the highest ranking mean value with 0.56; pct_NH_NHOPI_alone_CEN_2010 0.44; Aggregate_HH_INC_ACS_08_12  0.4\n",
    "\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.rgs_feature_sel, compare_rgs_featranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS Result for regression model in table 8 listed overall model fit 9,755 observation.  Degree of freedom/df 9,742, and fitting model with 13 predictors\n",
    "- If P>|t| is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship, correlation between the dependent (target) variable and the independent (predictor) variables.\n",
    "- R-squared 0.980 overall measure of the strength of association in the dependent (target) variable 'ALL_RATE_1112' can be explianed by the independent variables (34 predictors)\n",
    "\n",
    "Table 8. Regression: Summary of Ordinary Least Squares/OLS Result for the dependent variable at the top (ALL_RATE_1112) with the 13 selected features as predictor variables below in Parameter Estimates.\n",
    "\n",
    "                                     OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:          ALL_RATE_1112   R-squared:                       0.980\n",
    "    Model:                            OLS   Adj. R-squared:                  0.980\n",
    "    Method:                 Least Squares   F-statistic:                 3.647e+04\n",
    "    Date:                Sat, 12 May 2018   Prob (F-statistic):               0.00\n",
    "    Time:                        21:19:10   Log-Likelihood:                -38004.\n",
    "    No. Observations:                9755   AIC:                         7.603e+04\n",
    "    Df Residuals:                    9742   BIC:                         7.613e+04\n",
    "    Df Model:                          13                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    =================================================================================================\n",
    "                                        coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    -------------------------------------------------------------------------------------------------\n",
    "    pct_Tot_Occp_Units_CEN_2010       0.3504      0.011     31.446      0.000       0.329       0.372\n",
    "    pct_NH_NHOPI_alone_CEN_2010      -0.5520      0.495     -1.114      0.265      -1.523       0.419\n",
    "    Aggregate_HH_INC_ACS_08_12     1.664e-08   2.43e-09      6.843      0.000    1.19e-08    2.14e-08\n",
    "    pct_NH_AIAN_alone_ACS_08_12      -0.2153      0.017    -12.750      0.000      -0.248      -0.182\n",
    "    MBL_COHORT_1112                  -0.0033      0.000     -8.491      0.000      -0.004      -0.003\n",
    "    pct_Pop_Under_5_CEN_2010         -0.3329      0.081     -4.130      0.000      -0.491      -0.175\n",
    "    pct_Prs_Blw_Pov_Lev_ACS_08_12     0.0165      0.016      1.019      0.308      -0.015       0.048\n",
    "    pct_Age5p_WGerman_ACS_08_12       0.9944      0.261      3.809      0.000       0.483       1.506\n",
    "    pct_PUB_ASST_INC_ACS_08_12       -0.2389      0.054     -4.409      0.000      -0.345      -0.133\n",
    "    pct_Females_CEN_2010              1.0530      0.020     53.583      0.000       1.015       1.092\n",
    "    pct_Age5p_German_ACS_08_12       -0.7203      0.265     -2.719      0.007      -1.240      -0.201\n",
    "    Med_House_value_ACS_08_12       8.88e-06   1.26e-06      7.039      0.000    6.41e-06    1.14e-05\n",
    "    pct_NH_Blk_alone_CEN_2010        -0.1202      0.010    -12.347      0.000      -0.139      -0.101\n",
    "    ==============================================================================\n",
    "    Omnibus:                     1646.451   Durbin-Watson:                   1.818\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            27211.480\n",
    "    Skew:                           0.294   Prob(JB):                         0.00\n",
    "    Kurtosis:                      11.161   Cond. No.                     5.18e+08\n",
    "\n",
    "When the following increase, ALL_RATE_1112 would   (list first 3)\n",
    "- pct_Females_CEN_2010:  increase 1.05 \n",
    "- pct_Age5p_WGerman_ACS_08_12:  increase 0.99\n",
    "- pct_Tot_Occp_Units_CEN_2010:  increase 0.35\n",
    "\n",
    "When the following increase, ALL_RATE_1112 would  (list first 3)\n",
    "- pct_Age5p_German_ACS_08_12:  decrease 0.72 \n",
    "- pct_NH_NHOPI_alone_CEN_2010:  decrease 0.55 \n",
    "- pct_Pop_Under_5_CEN_2010:  decrease 0.33\n",
    "\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.rgs_stats, rgs_r2_featimportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Samples\n",
    "We used the training dataset to learn and evaluate predictive models.  The sklearn.model_selection.train_test_split API was used the target variable *ALL_RATE_1112* to split the dataset into 70% /6,828 observation for training, 30% / 2,927 observation for testing.  \n",
    ">Reference: API implementation: jtmoogle.hsgraduation.create_rgs_sample\n",
    "\n",
    "#### Detect Outliers\n",
    "Using John Tukey's Method [35] to identfy outliers data points by calculating as 1.5 times the interquartile range (IQR). A data point with a feature that was beyond an outlier step outside of the IQR for that feature is considered abnormal. \n",
    "\n",
    "#### Feature Scaling\n",
    "The natural logarithm was applied to scale the good_data, and we used heatmap to get further insight exhibited the feature's Correlation Matrix in Figure 7.  \n",
    "\n",
    "![Figure 7: Visualize comparing Regression data and log_data in Correlation Matrix](saved/rgs_corr_feat.scaling.png) Figure 7. Visualize comparing Regression data and log_data in Correlation Matrix. \n",
    "\n",
    "*Table 9 Feature Scaling: Log Transform change if any value above 0.5 *\n",
    "\n",
    "|Features in correlation above 0.5 | Value Change | Correlation Change|\n",
    "| --- | ---- | ----- |\n",
    "| Med_House_value_ACS_0812 & Aggregate_HH_INC_ACS_08_12 | 0.6 -> 0.6 | no change |\n",
    "| pct_PUB_ASST_INC_ACS_08_12 & pct_Prs_Blw_Pov_Lev_ACS_08_12 | 0.5 -> 0.4 | weaker correlation | \n",
    "| pct_NH_Blk_alone_CEN_2010 & MBL_COHORT_1112 | 0.2 -> 0.7 | Stronger |\n",
    "| pct_Age5p_German_ACE_08_12 & pct_Age5p_WGerman_ACS_08_12 | 0.0 -> .5 | Stronger | \n",
    "    \n",
    "#### Fit Predictive Model for Supervised Learning Algorithms\n",
    "We used the training dataset to learn and evaluate predictive models.  The process trained a set of predictive models respectively, and experimented the sklearn.linear_model, sklearn.naive_bay, sklearn.neighbors, sklearn.svm APIs as follows: \n",
    "- Linear Regression (Linear) used default value\n",
    "- Random Forest Regressor (RF) used 30 for max_depth, 2 for random_state\n",
    "- K-Nearest Neighbors Regressor (KNN) used 10 for n_neighbors=10\n",
    "- Decision Tree Regressor (DT) used default value\n",
    "- Ridge used 0.05 for alpha with 0.05\n",
    "- Support Vector Regression (SVR) used rbf for Kernel\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.rgs_r2_featimportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "The Linear Regression (Linear) was selected as Naive Prediction.  We compared the results of performance metrics with the Naive Prediction.  Time taken for training and testing were captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "We used Testing datase and the 10-fold cross-validation to test harness, compared the performance metrics exprimented various supervised learning algorithms.  The API sklearn.model_selection.KFold used 10 for n_splits, 99 for random_state. \n",
    "\n",
    "**Benchmark for Classification**\n",
    "- The output of API sklearn.model_selection.KFold showed average of classification accuracy, precision, recall, **f1-score**\n",
    "- The report of Confusion Matrix displayed the accuracy of a model with two or more classes.  The table presents predictions on the x-axis and accuracy outcomes on the y-axis. Predictions for 0 that were actually 0 appear in the cell for prediction=0 and actual=0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual=1. \n",
    "- Classification Report of API sklearn.metric listed the precision, recall, f1-score and support for each class.\n",
    "- If the model result had coef parameter, the output was in ascending order, the largest score the best score.  \n",
    "\n",
    "We used the Gaussian NB as Naive Prediction that predict all school district would have PASSED the goal of passing '900%' graduation rate. The accuracy was 0.55, and f1 score was 0.60.\n",
    "\n",
    "*Table 10 Classification Benchmark with 10 KFold cross validation, f1-score and Accurancy sorted by f1-score.*\n",
    "- Gaussian NB was Naive Prediction, accuracy 0.55, f1 score 0.60.  But it had the longest time taken to learn.\n",
    "- Gradient Boosting Classifier has the best performance, accuracy 0.75, f1 score 0.67\n",
    "\n",
    "|cls_names|train_time|test_time|accuracy_score|roc_auc|precision|avg_precision|recall|f1_score|\n",
    "| --------------- | --------------- | --------------- | ---- | ---- | ---- | ---- |---- |---- |\n",
    "|**Gradient Boosting Classifier** (Best Performance)|1.00|0.00|**0.75**|0.83|0.69|0.76|0.64|**0.67**|\n",
    "|Decision Tree Classifier|0.05|0.00|0.73|0.80|0.66|0.69|0.63|0.64|\n",
    "|Random Forest Classifier|0.09|1.00|0.73|0.81|0.69|0.73|0.57|0.62|\n",
    "|**GaussianNB** (Naive Prediction)|0.00|0.00|**0.55**|0.72|0.46|0.64|0.88|**0.60**|\n",
    "|KNeighborsClassifier|0.09|1.00|0.64|0.65|0.54|0.50|0.53|0.53|\n",
    "|LogisticRegression|0.09|0.00|0.70|0.73|0.69|0.66|0.43|0.53|\n",
    "|RidgeClassifier|0.00|0.00|0.68|0.70|0.67|0.62|0.35|0.46|\n",
    "|Perceptron|0.09|0.00|0.50|0.59|0.44|0.51|0.59|0.34|\n",
    "\n",
    "![Figure 7 Classification Benchmark](saved/cls_visual_benchmark.f1.png) Figure 7  Visualize classification benchmark with F1-score and Accurancy sorted by f1-score.  \n",
    "- Gradient Boosting Classifer had best f2 score and accuracy score, but its training time taken was the longest\n",
    "- GaussianNB has 4th beste of accuracy score and f1 score.\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.cls_visual_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark for Regression**\n",
    "- The output of API sklearn.model_selection.cross_val_score showed average of explained_variance_score, mean_absolute_error/MAE, mean_squared_error/MSE, root_mean_squared_error/RMSE, median_absolute_error/Median Abs Err, r2_score \n",
    "- If the model result had calculated root sqaure of MSE, the output was in ascending order, the largest score the best score.  If the success, the output would be 100, if school districts meet the goal of 90% graduation rate. \n",
    "\n",
    "We used the Linear Regression as Naive Prediction that predict all school district would have '100%' graduation rate. The RMSE was 10.71 and r2 score was 0.19.\n",
    "\n",
    "Table 11 Regression Benchmark with 10 KFold cross validation, RMSE and r2 score sorted by RMSE score.\n",
    "- Linear Regression was Naive Prediction, RMSE 10.71, r2 score 0.19\n",
    "- Random Forest Regressor has the best fit, RMSE 10.44, r2 score, 0.22\n",
    "\n",
    "|Regressor| Train<br>Time| Test<br>Time | Explained Variance| MAE | MSE| RMSE| Median Abs Err|R2 Score|\n",
    "| :--- | :---: |:---: |:---: |:---: |:---: |:---: |:---: |:---: |\n",
    "|**Random Forest Regressor** (Best Fit)|0.11|0.00|0.22|7.67|111.04|**10.54**|5.67|0.22|\n",
    "|Ridge|0.00|0.00|0.19|7.72|114.66|10.71|5.97|0.19|\n",
    "|**Linear Regression** (Naive Prediction)|0.00|0.00|0.19|7.72|114.71|**10.71**|5.96|0.19|\n",
    "|SVR|1.00|1.00|0.01|8.50|151.31|12.30|5.81|0.07|\n",
    "|K-Neighbors Regressor|0.01|0.01|0.17|9.53|166.07|12.89|7.07|0.17|\n",
    "|Decision Tree Regressor|0.02|0.00|0.37|9.74|198.05|14.07|6.80|0.39|\n",
    "\n",
    "![Figure 8 Regression Benchmark by RMSE](saved/rgs_visual_benchmark.rmse.png) Figure 8  Visualize Regression benchmark with RMSE \n",
    "- Random Forest Regressor has the best RMSE 10.44\n",
    "- Linear Regression has 4th best of RMSE 10.71\n",
    "\n",
    ">Reference: API implementation: jtmoogle.hsgraduation.rgs_visual_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification \n",
    "During capstone development, we experienced the following which cause us to reinstall software platform, or OS drivers updates\n",
    "- Changed to use new GPU card which was different manufactory from prior GPU, we need to uninstall and reinstall GPU drivers, as well as tensor flow too.\n",
    "- Anaconda major update, we need to re-install all dependent API packages, and tensor flow too\n",
    "\n",
    "In summary, the application is useful for large number of data elements (i.e. 500 data elements), capable to distinguish and keep the higher feature importance within acceptable timeframes (i.e. 15 minutes for processing 9k observations)  It allows to reduce to have 5% features (i.e. 25 features) which have higher feature importance.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this report, we demoed preliminary results for predicting High School Graduation from a large number of dataset including high school graduation and census records.  We experimented various feature selection algorithms, detected outliers, applied feature scaling, and compared for the best performance.  We found the Classification has better accuracy score than Regression result, and less features to fit and predict.  \n",
    "- Classification: 'Gradient Boosting Classifier' has the best performance, as it has the higher f1 score 0.67 than naive prediction Gaussian NB 0.60.  The accuracy was 0.75 better than 0.55.  \n",
    "- Regression model: 'Random Forest Regressor' has the best fit because it has the less RMSE 10.54 value than naive prediction Linear Model 10.71.  R2 0.22 better than 0.19\n",
    "\n",
    "\n",
    "We explored the following insights from the experimented result of supervised learning algorithms, and understood the factors related to increase percentage achieving the goal of 90% graduation rate.  \n",
    "\n",
    "**Regression**  \n",
    "The 'Success_Pass_90' target would increase, suggest if population and households within the school districts or neighborhoods,   \n",
    "- More households increasing median household income if our job opportunities increase and more high paid jobs\n",
    "- More family own the houses increasing percentage house owners live in the housing unit \n",
    "- More family persons 25 years and over with college degree \n",
    "- Decreasing not married couple in the household (reduce single parent percentage)\n",
    "- Decreasing percentage of female no husband (reduce divorce percentage)\n",
    "- Decreasing civilians aged 16 years and over and employed (reduce teenagers dropping out from high schools)\n",
    "\n",
    "**Classification**  \n",
    "If graduation rate would increase percentage, suggest if population and households within the school districts or neighborhoods,   \n",
    "- Increasing females population \n",
    "- Increasing people ages 5+ years who speak English less than \"very well\" and speak another West Germanic language at home\n",
    "- More family own the houses increasing percentage house owners live in the housing unit \n",
    "- Decreasing persons 5+ years who speak English less than \"very well\" and speak German at home\n",
    "- Decreasing non-Hispanic Native Hawaiian and Other Pacific Islander only\n",
    "- Decreasing number of persons under age 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Deployment\n",
    "All analysis performed in this study can be reproducible, and source files were on GitHub [jtmoogle](https://github.com/jtmoogle/capstone-report/jtmoogle) \n",
    "- Anaconda 3 (64-bit) \n",
    "- Program language: python 3 for data manipulation.   \n",
    "- tensorflow on CUDA GPU on Windows 10 \n",
    "- Tool: jupyter notebook for report and capstone analysis \n",
    "- Machine learning libraries: sckikt-learn/sklearn\n",
    "- Capstone Report in PDF format\n",
    "    \n",
    "### Improvement\n",
    "We would consider to make our jtmoogle APIs available to public, and benefit to broader audiences\n",
    "- load_gradcensus - load high school graduation data (jtmoogle/data/GRADUATION_WITH_CENSUS.csv)\n",
    "\n",
    "Using high school graduation data, we would expose\n",
    "- For Classfication APIs \n",
    "    - plot plot_cls_gradcensus - illustrate classification graduation census data\n",
    "    - preproc_cls_data - prepare classification data cleaning\n",
    "    - cls_feature_sel - classification feature selection\n",
    "    - compare_cls_featranking - compared classification feature ranking \n",
    "    - cls_stats - classification statistic summary\n",
    "    - cls_acc_featimportance - mean accuracy score and feature importance result for classification feature (7) create_cls_sample - create sample for Training and Testing datasets\n",
    "    - cls_acc_featimportance - calculated F score, accuracy, feature importance\n",
    "    - cls_visual_benchmark - benchmark result and visualization\n",
    "    - cls_pca - PCA result for classification\n",
    "\n",
    "- For Regression APIs using high school g\n",
    "    - plot_rgs_gradcensus - illustrate regression graduation census data\n",
    "    - preproc_rgs_data - prepare Regression data cleaning\n",
    "    - rgs_feature_sel - regression feature selection\n",
    "    - compare_rgs_featranking - compared regression feature ranking \n",
    "    - rgs_stats - regression statistic summary\n",
    "    - create_rgs_sample - create sample for Training and Testing datasets\n",
    "    - rgs_r2_featimportanct - R2 score and feature importance for regression features\n",
    "    - rgs_visual_benchmark - benchmark result and visualization\n",
    "    \n",
    "In addition, we could generated web solution\n",
    "- Allow school adminstratives, faculties and parents to see history the particular school district graduation rates\n",
    "- Allow people to change the selected features to predict school graduation rates.  This could help upcoming planning for school improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "We spent over 3 months to implement python APIs for this capstone project.\n",
    "\n",
    "Our thump up experience\n",
    "- The most time taken was to explore options for feature selections for classification and regression.  Experimented and troubleshoot several of supervising learning APIs and algorithms in python 3 because not of all of APIs are compatible to python 3.  \n",
    "- The flexibility we developed was to the configuration list, and to dynamically invoke reusable methods.  \n",
    "- The most handy was MyHelper.stats method skimpily generated common statistic output and plots.\n",
    "- The automation we'd love was runme method passing scenario number to execute classification model or/and regression model.\n",
    "\n",
    "Our thump down experience\n",
    "- The most frustration was to re-install Anaconda and enforce updating API packages several times due to incompatible API version updates or OS driver GPU hardware change and hard drive boot disk failure etc.\n",
    "- The unsolve task which we still could NOT make this work smoothly was to use jupyter converter to PDF file.  The output of PDF file was poorly generated and not acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] [The GradNation Campaign: Our Goal: Increase the nation’s on-time high school graduation rate to 90% for the class of 2020](http://gradnation.americaspromise.org/)  \n",
    "\n",
    "[2] [2015 Building a Grad Nation Report: Progress and Challenge in Ending the High School Dropout Epidemic](http://www.americaspromise.org/report/2015-building-grad-nation-report) \n",
    "<br> [Number of Additional Graduates Needed to Reach a 90 Percent Graduation Rate by State and Subgroup](http://www.americaspromise.org/resource/appendix-h-number-additional-graduates-needed-reach-90-percent-graduation-rate-state-and-0) \n",
    "\n",
    "[3] [An Intro to Data for Diploma by devpost.com](https://datafordiplomas.devpost.com/)<br>[Data for Diploma by devpost.com - Resources](https://datafordiplomas.devpost.com/details/resources)<br>[Data for Diploma by devpost.com - Submissions](https://datafordiplomas.devpost.com/submissions/search?utf8=%E2%9C%93) \n",
    "\n",
    "[4] [ML Predicts School Dropout Risk & Boosts Graduation Rates](https://blogs.technet.microsoft.com/machinelearning/2015/06/04/ml-predicts-school-dropout-risk-boosts-graduation-rates/)<br>[Predicting student dropout risks, increasing graduation rates with cloud analytics.](https://www.microsoft.com/en-us/microsoftservices/customerstories/Tacoma-Public-School-Uses-Predictive-Analytics.aspx)\n",
    "\n",
    "[5] [GeneLesinskiaStevenCornsbCihanDagli, \"Application of an Artificial Neural Network to Predict Graduation Success at the United States Military Academy\", Oct 2016](https://ac.els-cdn.com/S1877050916325212/1-s2.0-S1877050916325212-main.pdf?_tid=eeaf6b52-0640-11e8-8a0f-00000aab0f6b&acdnat=1517373838_c45b26fec8fc4461e78b251a3883631a) \n",
    "\n",
    "[6] [Stamos T. Karamouzis and Andreas Vrettos, \"n Artificial Neural Network for Predicting \n",
    "Student Graduation Outcomes \", Oct 2008](http://www.iaeng.org/publication/WCECS2008/WCECS2008_pp991-994.pdf)\n",
    "\n",
    "[7] [Suchita Borkar, K.Rajeswari, \"Attributes Selection for Predicting Students’ Academic \n",
    "Performance using Education Data Mining and Artificial Neural Network\"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.428.6445&rep=rep1&type=pdf)\n",
    "\n",
    "[8] [Making better use of graduation rates to assess school success](https://edsource.org/2016/making-better-use-of-graduation-rates-to-assess-school-success/562724) \n",
    "\n",
    "[9] Cited: Seabold, Skipper, and Josef Perktold. [Statsmodels: Econometric and statistical modeling with python.](http://www.statsmodels.org/stable/index.html) Proceedings of the 9th Python in Science Conference. 2010.\n",
    "\n",
    "[10] Cited: [Does scikit-learn have forward selection/stepwise regression algorithm?](https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm)\n",
    " \n",
    "[11] Sebastian Raschka, Aug 2014 [Predictive modeling, supervised machine learning, and pattern classification — the big picture](http://sebastianraschka.com/Articles/2014_intro_supervised_learning.html)\n",
    "\n",
    "[12] Cited: [Comparison of kernel ridge regression and SVR](http://scikit-learn.org/stable/auto_examples/plot_kernel_ridge_regression.html)\n",
    " 1. Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.  \n",
    " 2. API design for machine learning software: experiences from the scikit-learn project, Buitinck et al., 2013.  \n",
    "\n",
    "[13] Jason Brownlee, Ph.D. [How To Implement Baseline Machine Learning Algorithms From Scratch With Python](https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/)\n",
    "\n",
    "[14] [A Machine Learning Approach to Prioritizing Students at Risk of not Graduating High School on Time](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.725.9316&rep=rep1&type=pdf)\n",
    "\n",
    "[15] [Predicting STEM attrition using student transcript data](https://arxiv.org/pdf/1708.09344.pdf)\n",
    "\n",
    "[16] [Application of an Artificial Neural Network to Predict Graduation Success at the United States Military Academy, 2016, by Gene Lesinskia, Steven Cornsb, Cihan Dagli ](https://ac.els-cdn.com/S1877050916325212/1-s2.0-S1877050916325212-main.pdf?_tid=af483de2-f432-11e7-a7f6-00000aab0f02&acdnat=1515388598_9328bf843ca4ba17818d299d7d767c6a)\n",
    "\n",
    "[17] [7 tools in every data scientist’s toolbox](http://blog.datadive.net/7-tools-in-every-data-scientists-toolbox/)\n",
    "\n",
    "[18] [Big Data in Education 1.1 Introduction](https://www.youtube.com/watch?v=dc5Nx3tyR8g&feature=youtu.be)\n",
    "\n",
    "[19] [Data Science by Dr. Saed Sayad](http://www.saedsayad.com/data_mining_map.htm)\n",
    "\n",
    "[20] [Education in the United States](https://en.wikipedia.org/wiki/Education_in_the_United_States)\n",
    "\n",
    "[21] [GradNation leaders sound the alarm as U.S. remains off-track to reaching 90 percent](http://new.every1graduates.org/gradnation-leaders-sound-the-alarm-as-u-s-remains-off-track-to-reaching-90-percent/)\n",
    "\n",
    "[22] [Predictors of Postsecondary Success](https://ccrscenter.org/sites/default/files/CCRS%20Center_Predictors%20of%20Postsecondary%20Success_final_0.pdf)\n",
    "\n",
    "[23] [Progress and Challenge in Ending the High School Dropout Epidemic in 2015 Building a Grad Nation report](http://www.americaspromise.org/report/2015-building-grad-nation-report)\n",
    "\n",
    "[24] [Public High School Graduation Rates](https://nces.ed.gov/programs/coe/indicator_coi.asp)\n",
    "\n",
    "[25] [Stability Selection by Nicolai Meinshausen and Peter Buhlmann, in 2009](http://stat.ethz.ch/~nicolai/stability.pdf)\n",
    "\n",
    "[26] [Supervised and Unsupervised Machine Learning Algorithms](https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/)\n",
    "\n",
    "[27] [Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "\n",
    "[28] [U.S. Department of Education](https://www.ed.gov/news/press-releases/education-department-releases-new-school-level-graduation-rate-data-better-infor)\n",
    "\n",
    "[29] [Udacity MLND Capstone Project Description - Education](https://docs.google.com/document/d/1vjerjRQnWs1kLbZagDYT6rNqiwAG23Yj45oUY88IAxI/pub)\n",
    "\n",
    "[30] [Udacity Machine Learning Enginner Nanodegree Program](https://classroom.udacity.com/nanodegrees/nd009)\n",
    "\n",
    "[31] [Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    "\n",
    "[32] [Prediction of Graduation Delay Based on Student Characteristics and Performance](http://digitalrepository.unm.edu/cgi/viewcontent.cgi?article=1363&context=ece_etds)\n",
    "\n",
    "[33] [Selecting good features – Part IV: stability selection, RFE and everything side by side, Ando Saabas](http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/)\n",
    "\n",
    "[34] [Feature Selection methods with an example (or how to select the right variables?)](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)\n",
    "\n",
    "[35] (1) [Highlighting Outliers in your Data with the Tukey Method](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/) \n",
    "(2) [Outlier: Tukey's fences](https://en.wikipedia.org/wiki/Outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1: Python Source and Execution Result\n",
    "The jtmoogle APIs were developed for Predicting US High School Graduation Success capstone report.\n",
    "- The sources files for APIs were available in GitHub at [jtmoogle](https://github.com/jtmoogle/capstone-report/jtmoogle)\n",
    "- The [runme.pdf](https://github.com/jtmoogle/capstone-report/runme.pdf) listed execution steps and output result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 2: Fields from feature selection in the proposal phase [proposal data analysis](https://github.com/jtmoogle/capstone-proposal/blob/master/proposal-data-analysis.pdf)\n",
    "\n",
    "| Features | Classifier process| Regression process| Description | \n",
    "| :---- | :----- | :---- |\n",
    "|County.1|include|include|County ID Number |\n",
    "|CWD_COHORT_1112|include|| Number of children with disabilities in the graduation cohort |\n",
    "|leaid11|include|| Local Education Agency (district) NCES ID |\n",
    "|MAS_COHORT_1112|include||Number of Asian/Pacific Islander students in the graduation cohort |\n",
    "|MBL_COHORT_1112|include|include|Number of Black students in the graduation cohort|\n",
    "|MHI_COHORT_1112|include||Number of Hispanic students in the graduation cohort |\n",
    "|pct_Age5p_German_ACS_08_12||include| percentage of population ages 5 years and over who speak English less than \"very well\" and speak German at home.|\n",
    "|pct_Age5p_Navajo_ACS_08_12|include| include|percentage of population ages 5 years and over who speak English less than \"very well\" and speak Navajo at home|\n",
    "|pct_Age5p_OthPacIsl_ACS_08_12|include|| percentage of population ages 5 years and over who speak English less than \"very well\" and speak some other Pacific Island language at home|\n",
    "|pct_Age5p_Scandinav_ACS_08_12||include| percentage of population ages 5 years and over who speak English less than \"very well\" and speak a Scandinavian language at home|\n",
    "|pct_Age5p_WGerman_ACS_08_12||include| percentage of population ages 5 years and over who speak English less than \"very well\" and speak another West Germanic language at home|\n",
    "|pct_Census_UAA_CEN_2010|include|include| percentage of addresses was returned to the Census with the postal code \"Undeliverable as Addressed\"  |\n",
    "|pct_Civ_unemp_16p_ACS_08_12||include|percentage of civilians ages 16 years and over in the labor force that are unemployed Persons |\n",
    "|pct_College_ACS_08_12|include|include| percentage of population aged 25 years and over that have a college degree or higher|\n",
    "|pct_Female_No_HB_ACS_08_12|include|include|The percentage of all ACS occupied housing units with a female householder and no husband of householder present  |\n",
    "|pct_Females_CEN_2010||include|percentage of population that is female|\n",
    "|pct_HHD_PPL_Und_18_CEN_2010|include|include| percentage of all census occupied housing units where one or more people are ages 18 years or under|\n",
    "|pct_Hispanic_CEN_2010|include|include| percentage of total population that identify as \"Mexican\", \"Puerto Rican\", \"Cuban\", or \"another Hispanic, Latino, or Spanish origin\"|\n",
    "|pct_Inst_GQ_CEN_2010|include|include|percentage of Census population who live in group quarters and are primarily ineligible, unable, or unlikely to participate in labor force while residents |\n",
    "|pct_Males_CEN_2010|include|include|percentage of Census total population that is male Persons 2010 |\n",
    "|pct_MLT_U10p_ACS_08_12|include|include|percentage of all ACS housing units that are in a structure that contains 10 or more housing units |\n",
    "|pct_Mobile_Homes_ACS_08_12|include||percentage of all ACS housing units that are considered mobile homes|\n",
    "|pct_MrdCple_HHD_CEN_2010|include|| percentage of all Census occupied housing units where the householder and his or her spouse are listed as members of the same household; |\n",
    "|pct_NH_AIAN_alone_ACS_08_12|include|include|percentage of total population indicate no Hispanic origin and their only race as \"Asian Indian\", \"Chinese\", \"Filipino\", \"Korean\", \"Japanese\", \"Vietnamese\", or \"Other Asian\" |\n",
    "|pct_NH_AIAN_alone_CEN_2010||include| percentage of population indicate no Hispanic origin and their only race as \"American Indian or Alaska Native\" or report entries such as Navajo, Blackfeet, Inupiat, Yup'ik, or Central/South American Indian |\n",
    "|pct_NH_Blk_alone_CEN_2010|include|include|percentage of total population indicate no Hispanic origin and their only race as \"Black, African Am., or Negro\" or report entries such as African American, Kenyan, Nigerian, or Haitian |\n",
    "|pct_NH_NHOPI_alone_ACS_08_12|include|| percentage of the population that indicate no Hispanic origin and their only race as \"Native Hawaiian\", \"Guamanian or Chamorro\", \"Samoan\", or \"Other Pacific Islander\" |\n",
    "|pct_NH_NHOPI_alone_CEN_2010||include|percentage of the 2010 Census total population that indicate no Hispanic origin and their only race as \"Native Hawaiian\", \"Guamanian or Chamorro\", \"Samoan\", or \"Other Pacific Islander\" |\n",
    "|pct_NO_PH_SRVC_ACS_08_12||include| ercentage of ACS occupied housing units that do not have a working telephone and available service |\n",
    "|pct_No_Plumb_ACS_08_12||include|percentage of all ACS housing units that do not have complete plumbing facilities  |\n",
    "|pct_Pop_25_44_CEN_2010||include| percentage of the 2010 Census total population that is between 25 and 44 years old|\n",
    "|pct_Pop_45_64_ACS_08_12||include| percentage of the ACS population that is between 45 and 64 years old |\n",
    "|pct_Pop_45_64_CEN_2010|include||percentage of the Census population that is between 45 and 64 years old |\n",
    "|pct_Pop_5_17_CEN_2010|include||percentage of the Census population that is between 5 and 17 years old |\n",
    "|pct_Pop_Under_5_CEN_2010|include|include|percentage of Census population that is under under 5 years old |\n",
    "|pct_Prs_Blw_Pov_Lev_ACS_08_12|include|include| percentage of the ACS eligible population that are classified as below the poverty level given their total family or household income|\n",
    "|pct_PUB_ASST_INC_ACS_08_12||include|  percentage of all ACS occupied housing units that receive public assistance income|\n",
    "|pct_Rel_Under_6_CEN_2010||include| percentage of 2010 Census family-occupied housing units with a related child under 6 years old|\n",
    "|pct_Sngl_Prns_HHD_CEN_2010|include|| percentage of all 2010 Census occupied housing units where a householder lives alone |\n",
    "|pct_TEA_Update_Leave_CEN_2010|include|include| percentage of addresses from which a Census form was expected to be delivered for mail return that were in an Update/Leave type of enumeration area in the 2010 Census |\n",
    "|pct_URBAN_CLUSTER_POP_CEN_2010||include|percentage of the 2010 Census total population that lives in a densely settled area containing 2,500 to 49,999 people |\n",
    "|pct_Vacant_CEN_2010||include| percentage of addresses in a 2010 Census mailback area that were confirmed as vacant housing units |\n",
    "|pct_Vacant_Units_ACS_08_12|include|| percentage of all ACS housing units where no one is living regularly at the time of interview; units occupied at the time of interview entirely by persons who are staying two months or less and who have a more permanent residence elsewhere are classified as vacant |\n",
    "|PUB_ASST_INC_ACS_08_12||include|Number of ACS households that receive public assistance income|\n",
    "|State.1||include| State ID Number |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For any questions or questions, please contact jtmoogle@gmail.com"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
